"""
Ù…Ø§Ú˜ÙˆÙ„ ØªØ´Ø®ÛŒØµ ØªÙ‚Ù„Ø¨ Ù¾ÛŒØ´Ø±ÙØªÙ‡
Ø§ÛŒÙ† Ù…Ø§Ú˜ÙˆÙ„ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªÙˆÚ©Ù†â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡ØŒ ØªÙ‚Ù„Ø¨ Ø±Ø§ ØªØ´Ø®ÛŒØµ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.
"""

import os
from typing import List, Dict, Tuple, Optional, Set
from difflib import SequenceMatcher
from collections import defaultdict
import tokenizer
import config

# ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ import networkx Ø¨Ø±Ø§ÛŒ clustering
try:
    import networkx as nx
    NETWORKX_AVAILABLE = True
except ImportError:
    NETWORKX_AVAILABLE = False


class PlagiarismDetector:
    """Ú©Ù„Ø§Ø³ ØªØ´Ø®ÛŒØµ ØªÙ‚Ù„Ø¨"""
    
    def __init__(self, template_tokens: Optional[str] = None):
        """
        Args:
            template_tokens: Ø±Ø´ØªÙ‡ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ú©Ø¯ Ù‚Ø§Ù„Ø¨ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
        """
        self.tokenizer = tokenizer.CTokenizer()
        self.similarity_cache: Dict[Tuple[str, str], float] = {}  # Ú©Ø´ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ø§Øª ØªÚ©Ø±Ø§Ø±ÛŒ
        self.template_tokens = template_tokens  # ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ú©Ø¯ Ù‚Ø§Ù„Ø¨
    
    def _calculate_similarity(self, token_str1: str, token_str2: str) -> float:
        """
        Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª Ø¨ÛŒÙ† Ø¯Ùˆ Ø±Ø´ØªÙ‡ ØªÙˆÚ©Ù†
        
        Args:
            token_str1: Ø±Ø´ØªÙ‡ ØªÙˆÚ©Ù† Ø§ÙˆÙ„
            token_str2: Ø±Ø´ØªÙ‡ ØªÙˆÚ©Ù† Ø¯ÙˆÙ…
        
        Returns:
            Ø¯Ø±ØµØ¯ Ø´Ø¨Ø§Ù‡Øª (0 ØªØ§ 100)
        """
        if not token_str1 or not token_str2:
            return 0.0
        
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©Ø´ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù…Ø­Ø§Ø³Ø¨Ø§Øª ØªÚ©Ø±Ø§Ø±ÛŒ
        cache_key = tuple(sorted([token_str1, token_str2]))
        if cache_key in self.similarity_cache:
            return self.similarity_cache[cache_key]
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª Ø¨Ø§ SequenceMatcher
        similarity = SequenceMatcher(None, token_str1, token_str2).ratio()
        similarity_percent = similarity * 100.0
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ú©Ø´
        self.similarity_cache[cache_key] = similarity_percent
        
        return similarity_percent
    
    def _is_valid_for_comparison(self, file_path: str) -> Tuple[bool, Optional[str]]:
        """
        Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø¢ÛŒØ§ ÙØ§ÛŒÙ„ Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª Ùˆ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø¢Ù† Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯
        
        Args:
            file_path: Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„
        
        Returns:
            ØªØ§Ù¾Ù„ (is_valid, token_string) - Ø§Ú¯Ø± Ù…Ø¹ØªØ¨Ø± Ù†Ø¨Ø§Ø´Ø¯ token_string=None
        """
        if not os.path.exists(file_path):
            return (False, None)
        
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                code = f.read()
            
            # Ø¨Ø±Ø±Ø³ÛŒ ØªØ¹Ø¯Ø§Ø¯ ØªÙˆÚ©Ù† (Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ: Ù‚Ø¨Ù„ Ø§Ø² ØªÙˆÚ©Ù†â€ŒØ³Ø§Ø²ÛŒ Ú©Ø§Ù…Ù„)
            token_count = self.tokenizer.get_token_count(code)
            if token_count < config.Config.MIN_TOKEN_COUNT:
                return (False, None)
            
            # ØªÙˆÚ©Ù†â€ŒØ³Ø§Ø²ÛŒ
            token_string = self.tokenizer.tokenize(code)
            
            # Ø­Ø°Ù ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ù‚Ø§Ù„Ø¨ (Template Subtraction)
            if self.template_tokens and token_string:
                # Ø­Ø°Ù ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ù‚Ø§Ù„Ø¨ Ø§Ø² Ú©Ø¯ Ø¯Ø§Ù†Ø´Ø¬Ùˆ
                # Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ: Ø­Ø°Ù Ø²ÛŒØ±Ø±Ø´ØªÙ‡â€ŒÙ‡Ø§ÛŒ Ù…Ø´ØªØ±Ú©
                # Ø§ÛŒÙ† ÛŒÚ© Ø±ÙˆØ´ Ø³Ø§Ø¯Ù‡ Ø§Ø³Øª - Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø¨Ø§ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡â€ŒØªØ± Ø¨Ù‡Ø¨ÙˆØ¯ Ø¯Ø§Ø¯
                student_tokens = token_string
                template_tokens = self.template_tokens
                
                # Ø§Ú¯Ø± ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ù‚Ø§Ù„Ø¨ Ø¯Ø± Ú©Ø¯ Ø¯Ø§Ù†Ø´Ø¬Ùˆ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ØŒ Ø­Ø°Ù Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
                # Ø§ÛŒÙ† Ú©Ø§Ø± Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¨Ø§Ø²Ú¯Ø´ØªÛŒ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ… ØªØ§ ØªÙ…Ø§Ù… Ù…ÙˆØ§Ø±Ø¯ Ø­Ø°Ù Ø´ÙˆÙ†Ø¯
                while template_tokens in student_tokens:
                    student_tokens = student_tokens.replace(template_tokens, '', 1)
                
                token_string = student_tokens
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø¬Ø¯Ø¯ ØªØ¹Ø¯Ø§Ø¯ ØªÙˆÚ©Ù† Ø¨Ø¹Ø¯ Ø§Ø² Ø­Ø°Ù Ù‚Ø§Ù„Ø¨
            if len(token_string) < config.Config.MIN_TOKEN_COUNT:
                return (False, None)
            
            return (True, token_string)
        except Exception:
            return (False, None)
    
    def compare_two_files(self, file1_path: str, file2_path: str) -> float:
        """
        Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¯Ùˆ ÙØ§ÛŒÙ„ Ùˆ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª
        
        Args:
            file1_path: Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ Ø§ÙˆÙ„
            file2_path: Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ Ø¯ÙˆÙ…
        
        Returns:
            Ø¯Ø±ØµØ¯ Ø´Ø¨Ø§Ù‡Øª (0 ØªØ§ 100)
        """
        # Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø¹ØªØ¨Ø§Ø± Ùˆ Ø¯Ø±ÛŒØ§ÙØª ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ (Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ: ÙÙ‚Ø· ÛŒÚ© Ø¨Ø§Ø± ØªÙˆÚ©Ù†â€ŒØ³Ø§Ø²ÛŒ)
        is_valid1, token_str1 = self._is_valid_for_comparison(file1_path)
        is_valid2, token_str2 = self._is_valid_for_comparison(file2_path)
        
        if not is_valid1 or not is_valid2 or not token_str1 or not token_str2:
            return 0.0
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª
        similarity = self._calculate_similarity(token_str1, token_str2)
        
        return similarity
    
    def detect_plagiarism_in_question(self, question_dir: str, question_num: int) -> List[Dict]:
        """
        ØªØ´Ø®ÛŒØµ ØªÙ‚Ù„Ø¨ Ø¯Ø± ÛŒÚ© Ø³ÙˆØ§Ù„ Ø®Ø§Øµ
        
        Args:
            question_dir: Ù…Ø³ÛŒØ± Ù¾ÙˆØ´Ù‡ Ø³ÙˆØ§Ù„
            question_num: Ø´Ù…Ø§Ø±Ù‡ Ø³ÙˆØ§Ù„
        
        Returns:
            Ù„ÛŒØ³Øª Ù…ÙˆØ§Ø±Ø¯ ØªÙ‚Ù„Ø¨ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡
        """
        plagiarism_cases = []
        
        if not os.path.exists(question_dir):
            return plagiarism_cases
        
        # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ ØªÙ…Ø§Ù… ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù† (Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ: ÙÙ‚Ø· ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…Ø¹ØªØ¨Ø±)
        student_files = {}
        for file_name in os.listdir(question_dir):
            if file_name.endswith('.c'):
                student_id = os.path.splitext(file_name)[0]
                file_path = os.path.join(question_dir, file_name)
                
                is_valid, _ = self._is_valid_for_comparison(file_path)
                if is_valid:
                    student_files[student_id] = file_path
        
        # Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¬ÙØªÛŒ ØªÙ…Ø§Ù… ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
        student_ids = list(student_files.keys())
        total_comparisons = len(student_ids) * (len(student_ids) - 1) // 2
        
        print(f"  ğŸ” Ø¯Ø± Ø­Ø§Ù„ Ù…Ù‚Ø§ÛŒØ³Ù‡ {total_comparisons} Ø¬ÙØª ÙØ§ÛŒÙ„ Ø¨Ø±Ø§ÛŒ Ø³ÙˆØ§Ù„ {question_num}...")
        
        comparison_count = 0
        for i in range(len(student_ids)):
            for j in range(i + 1, len(student_ids)):
                student1_id = student_ids[i]
                student2_id = student_ids[j]
                
                file1_path = student_files[student1_id]
                file2_path = student_files[student2_id]
                
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª
                similarity = self.compare_two_files(file1_path, file2_path)
                
                # Ø§Ú¯Ø± Ø´Ø¨Ø§Ù‡Øª Ø§Ø² Ø¢Ø³ØªØ§Ù†Ù‡ Ø¨ÛŒØ´ØªØ± Ø¨Ø§Ø´Ø¯
                if similarity >= config.Config.SIMILARITY_THRESHOLD:
                    plagiarism_cases.append({
                        'question': question_num,
                        'student1': student1_id,
                        'student2': student2_id,
                        'similarity': similarity,
                        'file1': file1_path,
                        'file2': file2_path
                    })
                
                comparison_count += 1
                if comparison_count % 50 == 0:
                    print(f"    âœ“ {comparison_count}/{total_comparisons} Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯...")
        
        return plagiarism_cases
    
    def detect_plagiarism_all_questions(self, output_dir: str) -> List[Dict]:
        """
        ØªØ´Ø®ÛŒØµ ØªÙ‚Ù„Ø¨ Ø¯Ø± ØªÙ…Ø§Ù… Ø³ÙˆØ§Ù„Ø§Øª
        
        Args:
            output_dir: Ù…Ø³ÛŒØ± Ù¾ÙˆØ´Ù‡ Ø®Ø±ÙˆØ¬ÛŒ
        
        Returns:
            Ù„ÛŒØ³Øª ØªÙ…Ø§Ù… Ù…ÙˆØ§Ø±Ø¯ ØªÙ‚Ù„Ø¨
        """
        all_plagiarism_cases = []
        
        print("\n" + "="*60)
        print("ğŸ” Ù…Ø±Ø­Ù„Ù‡ ØªØ´Ø®ÛŒØµ ØªÙ‚Ù„Ø¨")
        print("="*60)
        
        for question_num in range(1, config.Config.NUM_QUESTIONS + 1):
            question_dir = os.path.join(output_dir, f"Q{question_num}")
            
            if os.path.exists(question_dir):
                print(f"\nğŸ“ Ø¨Ø±Ø±Ø³ÛŒ Ø³ÙˆØ§Ù„ {question_num}...")
                cases = self.detect_plagiarism_in_question(question_dir, question_num)
                all_plagiarism_cases.extend(cases)
                
                if cases:
                    print(f"  âš  {len(cases)} Ù…ÙˆØ±Ø¯ ØªÙ‚Ù„Ø¨ Ø§Ø­ØªÙ…Ø§Ù„ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯")
                else:
                    print(f"  âœ“ Ù‡ÛŒÚ† Ù…ÙˆØ±Ø¯ ØªÙ‚Ù„Ø¨ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù†Ø´Ø¯")
        
        return all_plagiarism_cases
    
    def find_clusters(self, plagiarism_cases: List[Dict]) -> List[Dict]:
        """
        Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙ‚Ù„Ø¨ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú¯Ø±Ø§Ù (Connected Components)
        
        Args:
            plagiarism_cases: Ù„ÛŒØ³Øª Ù…ÙˆØ§Ø±Ø¯ ØªÙ‚Ù„Ø¨
        
        Returns:
            Ù„ÛŒØ³Øª Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ (Ù‡Ø± Ø®ÙˆØ´Ù‡ Ø´Ø§Ù…Ù„ Ù„ÛŒØ³Øª Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù†)
        """
        if not NETWORKX_AVAILABLE:
            # Ø§Ú¯Ø± networkx Ù†ØµØ¨ Ù†Ø´Ø¯Ù‡ØŒ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø¯Ù‡ Ø¨Ø§ dictionary Ù…ÛŒâ€ŒØ³Ø§Ø²ÛŒÙ…
            return self._find_clusters_simple(plagiarism_cases)
        
        # Ø³Ø§Ø®Øª Ú¯Ø±Ø§Ù
        G = nx.Graph()
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ÛŒØ§Ù„â€ŒÙ‡Ø§ (edge) Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø¬ÙØª Ù…ØªÙ‚Ù„Ø¨
        for case in plagiarism_cases:
            student1 = case['student1']
            student2 = case['student2']
            G.add_edge(student1, student2, similarity=case['similarity'], question=case['question'])
        
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø§Ø¬Ø²Ø§ÛŒ Ù…ØªØµÙ„ (Connected Components)
        clusters = []
        for component in nx.connected_components(G):
            if len(component) > 1:  # ÙÙ‚Ø· Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ø­Ø¯Ø§Ù‚Ù„ 2 Ù†ÙØ±
                cluster_students = sorted(list(component))
                clusters.append({
                    'students': cluster_students,
                    'size': len(cluster_students),
                    'cluster_id': len(clusters) + 1
                })
        
        return clusters
    
    def _find_clusters_simple(self, plagiarism_cases: List[Dict]) -> List[Dict]:
        """
        Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ Ø¨Ø¯ÙˆÙ† networkx (fallback)
        
        Args:
            plagiarism_cases: Ù„ÛŒØ³Øª Ù…ÙˆØ§Ø±Ø¯ ØªÙ‚Ù„Ø¨
        
        Returns:
            Ù„ÛŒØ³Øª Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§
        """
        # Ø³Ø§Ø®Øª ÛŒÚ© dictionary Ø¨Ø±Ø§ÛŒ Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ Ø§Ø±ØªØ¨Ø§Ø·Ø§Øª
        connections: Dict[str, Set[str]] = defaultdict(set)
        
        for case in plagiarism_cases:
            student1 = case['student1']
            student2 = case['student2']
            connections[student1].add(student2)
            connections[student2].add(student1)
        
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ DFS Ø³Ø§Ø¯Ù‡
        visited: Set[str] = set()
        clusters = []
        cluster_id = 1
        
        def dfs(student: str, current_cluster: Set[str]):
            """DFS Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ØªÙ…Ø§Ù… Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù† Ù…Ø±ØªØ¨Ø·"""
            if student in visited:
                return
            visited.add(student)
            current_cluster.add(student)
            
            for connected in connections[student]:
                if connected not in visited:
                    dfs(connected, current_cluster)
        
        for student in connections:
            if student not in visited:
                cluster = set()
                dfs(student, cluster)
                if len(cluster) > 1:
                    clusters.append({
                        'students': sorted(list(cluster)),
                        'size': len(cluster),
                        'cluster_id': cluster_id
                    })
                    cluster_id += 1
        
        return clusters
    
    def get_statistics(self, plagiarism_cases: List[Dict]) -> Dict:
        """
        Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ù…Ø§Ø± ØªÙ‚Ù„Ø¨
        
        Args:
            plagiarism_cases: Ù„ÛŒØ³Øª Ù…ÙˆØ§Ø±Ø¯ ØªÙ‚Ù„Ø¨
        
        Returns:
            Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¢Ù…Ø§Ø±
        """
        stats = {
            'total_cases': len(plagiarism_cases),
            'by_question': defaultdict(int),
            'by_student': defaultdict(int),
            'similarity_distribution': {
                '85-90': 0,
                '90-95': 0,
                '95-99': 0,
                '99-100': 0
            },
            'clusters': []
        }
        
        for case in plagiarism_cases:
            # Ø¢Ù…Ø§Ø± Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³ÙˆØ§Ù„
            stats['by_question'][case['question']] += 1
            
            # Ø¢Ù…Ø§Ø± Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø§Ù†Ø´Ø¬Ùˆ
            stats['by_student'][case['student1']] += 1
            stats['by_student'][case['student2']] += 1
            
            # ØªÙˆØ²ÛŒØ¹ Ø´Ø¨Ø§Ù‡Øª
            similarity = case['similarity']
            if 85 <= similarity < 90:
                stats['similarity_distribution']['85-90'] += 1
            elif 90 <= similarity < 95:
                stats['similarity_distribution']['90-95'] += 1
            elif 95 <= similarity < 99:
                stats['similarity_distribution']['95-99'] += 1
            elif similarity >= 99:
                stats['similarity_distribution']['99-100'] += 1
        
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§
        clusters = self.find_clusters(plagiarism_cases)
        stats['clusters'] = clusters
        
        return stats


def load_template_tokens(template_path: Optional[str]) -> Optional[str]:
    """
    Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ ØªÙˆÚ©Ù†â€ŒØ³Ø§Ø²ÛŒ ÙØ§ÛŒÙ„ Ù‚Ø§Ù„Ø¨
    
    Args:
        template_path: Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ Ù‚Ø§Ù„Ø¨
    
    Returns:
        Ø±Ø´ØªÙ‡ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ù‚Ø§Ù„Ø¨ ÛŒØ§ None
    """
    if not template_path or not os.path.exists(template_path):
        return None
    
    try:
        return tokenizer.tokenize_file(template_path)
    except Exception as e:
        print(f"âš  Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙØ§ÛŒÙ„ Ù‚Ø§Ù„Ø¨: {str(e)}")
        return None


def detect_plagiarism(output_dir: str, template_path: Optional[str] = None) -> Tuple[List[Dict], Dict]:
    """
    ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ ØªÙ‚Ù„Ø¨
    
    Args:
        output_dir: Ù…Ø³ÛŒØ± Ù¾ÙˆØ´Ù‡ Ø®Ø±ÙˆØ¬ÛŒ
        template_path: Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ Ù‚Ø§Ù„Ø¨ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
    
    Returns:
        ØªØ§Ù¾Ù„ (Ù„ÛŒØ³Øª Ù…ÙˆØ§Ø±Ø¯ ØªÙ‚Ù„Ø¨, Ø¢Ù…Ø§Ø±)
    """
    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ù‚Ø§Ù„Ø¨
    template_tokens = load_template_tokens(template_path)
    if template_tokens:
        print(f"âœ“ ÙØ§ÛŒÙ„ Ù‚Ø§Ù„Ø¨ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯: {template_path}")
    
    detector = PlagiarismDetector(template_tokens=template_tokens)
    plagiarism_cases = detector.detect_plagiarism_all_questions(output_dir)
    statistics = detector.get_statistics(plagiarism_cases)
    
    # Ù†Ù…Ø§ÛŒØ´ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§
    if statistics['clusters']:
        print(f"\nğŸ“Š {len(statistics['clusters'])} Ø®ÙˆØ´Ù‡ ØªÙ‚Ù„Ø¨ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯:")
        for cluster in statistics['clusters']:
            print(f"  â€¢ Cluster #{cluster['cluster_id']}: {cluster['size']} Ø¯Ø§Ù†Ø´Ø¬Ùˆ - {', '.join(cluster['students'])}")
    
    return plagiarism_cases, statistics

